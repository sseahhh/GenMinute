
import chromadb
import os
import re
import logging
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma

from langchain_classic.retrievers.self_query.base import SelfQueryRetriever
from langchain_classic.chains.query_constructor.base import AttributeInfo

# í…ìŠ¤íŠ¸ ë¶„í• ì„ ìœ„í•œ import (ì˜ë¯¸ì  ì²­í‚¹ ëŒ€ì•ˆ)
from langchain_text_splitters import RecursiveCharacterTextSplitter
import numpy as np

from config import config

logger = logging.getLogger(__name__)

class VectorDBManager:
    _instance = None
    _initialized = False

    COLLECTION_NAMES = {
        'chunks': 'meeting_chunks',
        'subtopic': 'meeting_subtopic',
    }

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, persist_directory="./database/vector_db", upload_folder="./uploads", db_manager=None):
        if self._initialized:
            return
        if not config.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.client = chromadb.PersistentClient(path=persist_directory)
        self.embedding_function = OpenAIEmbeddings()
        self.upload_folder = upload_folder

        # DatabaseManager ì¸ìŠ¤í„´ìŠ¤ (ì™¸ë¶€ì—ì„œ ì£¼ì…ë°›ìŒ, SQLite ì‚­ì œë¥¼ ìœ„í•´)
        self.db_manager = db_manager

        # Initialize LLM for SelfQueryRetriever
        self.llm = ChatOpenAI(api_key=config.OPENAI_API_KEY, temperature=0)

        self.vectorstores = {
            key: Chroma(
                client=self.client,
                collection_name=name,
                embedding_function=self.embedding_function,
            )
            for key, name in self.COLLECTION_NAMES.items()
        }

        # Define metadata field information for SelfQueryRetriever
        self.metadata_field_infos = {
            "chunks": [
                AttributeInfo(name="dialogue_id", description="The unique identifier for the dialogue within the meeting", type="string"),
                AttributeInfo(name="chunk_index", description="The index of the chunk within the meeting", type="integer"),
                AttributeInfo(
                    name="title",
                    description="íšŒì˜ ì œëª© ë˜ëŠ” ë…¸íŠ¸ ì´ë¦„. ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ íšŒì˜/ë…¸íŠ¸ ì´ë¦„ì´ ì–¸ê¸‰ë˜ë©´ ì´ í•„ë“œë¡œ í•„í„°ë§. ë¶€ë¶„ ì¼ì¹˜ ê²€ìƒ‰ ê°€ëŠ¥. ì˜ˆ: ì¿¼ë¦¬ì— 'ì‚¬ìíšŒë‹´'ì´ ìˆìœ¼ë©´ titleì— 'ì‚¬ìíšŒë‹´'ì´ í¬í•¨ëœ ëª¨ë“  ë¬¸ì„œ",
                    type="string"
                ),
                AttributeInfo(
                    name="meeting_date",
                    description="íšŒì˜ ì‹œì ì„ ë‚˜íƒ€ë‚´ëŠ” í…ìŠ¤íŠ¸ í•„ë“œ (ì˜ˆ: '2025-11-07'). ë¬¸ìì—´ë¡œ ì €ì¥ë˜ë©° ì •í™•í•œ ì¼ì¹˜ ê²€ìƒ‰ì— ì‚¬ìš©ë©ë‹ˆë‹¤",
                    type="string"
                ),
                AttributeInfo(name="audio_file", description="The name of the audio file for the meeting", type="string"),
                AttributeInfo(name="start_time", description="The start time of the chunk in seconds", type="float"),
                AttributeInfo(name="end_time", description="The end time of the chunk in seconds", type="float"),
                AttributeInfo(name="speaker_count", description="The number of different speakers in the chunk", type="integer"),
            ],
            "subtopic": [
                AttributeInfo(
                    name="meeting_title",
                    description="íšŒì˜ ì œëª© ë˜ëŠ” ë…¸íŠ¸ ì´ë¦„. ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ íšŒì˜/ë…¸íŠ¸ ì´ë¦„ì´ ì–¸ê¸‰ë˜ë©´ ì´ í•„ë“œë¡œ í•„í„°ë§. ë¶€ë¶„ ì¼ì¹˜ ê²€ìƒ‰ ê°€ëŠ¥. ì˜ˆ: ì¿¼ë¦¬ì— 'ì‚¬ìíšŒë‹´'ì´ ìˆìœ¼ë©´ meeting_titleì— 'ì‚¬ìíšŒë‹´'ì´ í¬í•¨ëœ ëª¨ë“  ë¬¸ì„œ",
                    type="string"
                ),
                AttributeInfo(
                    name="meeting_date",
                    description="íšŒì˜ ì‹œì ì„ ë‚˜íƒ€ë‚´ëŠ” í…ìŠ¤íŠ¸ í•„ë“œ (ì˜ˆ: '2025-11-07'). ë¬¸ìì—´ë¡œ ì €ì¥ë˜ë©° ì •í™•í•œ ì¼ì¹˜ ê²€ìƒ‰ì— ì‚¬ìš©ë©ë‹ˆë‹¤",
                    type="string"
                ),
                AttributeInfo(name="audio_file", description="The name of the audio file for the meeting", type="string"),
                AttributeInfo(name="main_topic", description="The main topic of the summarized sub-chunk", type="string"),
                AttributeInfo(name="summary_index", description="The index of the summary sub-chunk", type="integer"),
            ],
        }

        # Define document content descriptions for SelfQueryRetriever
        self.document_content_descriptions = {
            "chunks": "íšŒì˜ ëŒ€í™” ë‚´ìš©ì˜ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ê·¸ë£¹í™”ëœ ì²­í¬ (í™”ì ë¼ë²¨ ë° íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)",
            "subtopic": "íšŒì˜ë¡ì˜ ìš”ì•½ëœ í•˜ìœ„ ì£¼ì œ",
        }

        logger.info(f"âœ… VectorDBManager for collections {list(self.COLLECTION_NAMES.values())} initialized.")

        self._initialized = True

    def _clean_text(self, formatted_text: str) -> str:
        """
        ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•´ì„œ [Speaker X, MM:SS] í˜•ì‹ì˜ ì •ë³´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.

        Args:
            formatted_text (str): [Speaker X, MM:SS] í˜•ì‹ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸

        Returns:
            str: ìˆœìˆ˜í•œ ëŒ€í™” í…ìŠ¤íŠ¸ (speakerì™€ ì‹œê°„ ì •ë³´ ì œê±°)
        """
        # [Speaker X, MM:SS] íŒ¨í„´ ì œê±°
        # [^,]+ : ì‰¼í‘œê°€ ì•„ë‹Œ ë¬¸ì (ìˆ«ì, "Unknown" ë“±)
        # \d{2}:\d{2} : MM:SS í˜•ì‹
        pattern = r'\[Speaker [^,]+, \d{2}:\d{2}\]\s*'
        cleaned_text = re.sub(pattern, '', formatted_text)

        # ë¹ˆ ì¤„ ì œê±°
        cleaned_text = '\n'.join(line for line in cleaned_text.split('\n') if line.strip())

        return cleaned_text.strip()

    def add_meeting_as_chunk(self, meeting_id, title, meeting_date, audio_file, segments):
        """
        íšŒì˜ ëŒ€í™” ë‚´ìš©ì„ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ì²­í¬ë¡œ ë¬¶ì–´ DBì— ì €ì¥í•©ë‹ˆë‹¤.
        í™”ì ë³€ê²½, ì‹œê°„ ê°„ê²©ì„ ê³ ë ¤í•˜ì—¬ ì²­í‚¹í•˜ë©°, Geminië¥¼ ì‚¬ìš©í•´ì„œ speakerì™€ ì‹œê°„ ì •ë³´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.

        Args:
            meeting_id (str): íšŒì˜ ID
            title (str): íšŒì˜ ì œëª©
            meeting_date (str): íšŒì˜ ì¼ì‹œ
            audio_file (str): ì˜¤ë””ì˜¤ íŒŒì¼ëª…
            segments (list): íšŒì˜ ëŒ€í™” ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
                ê° ì„¸ê·¸ë¨¼íŠ¸ëŠ” {'speaker_label', 'start_time', 'segment', ...} í¬í•¨
        """
        chunk_vdb = self.vectorstores['chunks']

        try:
            # 1. ìŠ¤ë§ˆíŠ¸ ì²­í‚¹: í™”ì ë³€ê²½ê³¼ ì‹œê°„ ê°„ê²©ì„ ê³ ë ¤
            chunks = self._create_smart_chunks(segments, max_chunk_size=1000, time_gap_threshold=60)

            logger.info(f"ğŸ“¦ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ìœ¼ë¡œ {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ")

            # 2. ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ê° ì²­í¬ì˜ í…ìŠ¤íŠ¸ ì •ì œ (speakerì™€ ì‹œê°„ ì •ë³´ ì œê±°)
            logger.info(f"ğŸ”§ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì •ì œ ì¤‘...")
            for chunk in chunks:
                chunk['text'] = self._clean_text(chunk['text'])

            # 3. ê° ì²­í¬ë¥¼ Vector DBì— ì €ì¥
            chunk_texts = []
            chunk_metadatas = []
            chunk_ids = []

            for i, chunk_info in enumerate(chunks):
                chunk_texts.append(chunk_info['text'])
                # meeting_dateë¥¼ ë¬¸ìì—´ë¡œ ê°•ì œ ë³€í™˜ (datetime ê°ì²´ì¼ ê²½ìš° ëŒ€ë¹„)
                meeting_date_str = str(meeting_date) if meeting_date else ""
                chunk_metadatas.append({
                    "meeting_id": meeting_id,
                    "dialogue_id": f"{meeting_id}_chunk_{i}",
                    "chunk_index": i,
                    "title": title,
                    "meeting_date": meeting_date_str,
                    "audio_file": audio_file,
                    "start_time": chunk_info['start_time'],
                    "end_time": chunk_info['end_time'],
                    "speaker_count": chunk_info['speaker_count']
                })
                chunk_ids.append(f"{meeting_id}_chunk_{i}")

            # Vector DBì— ì¶”ê°€
            chunk_vdb.add_texts(
                texts=chunk_texts,
                metadatas=chunk_metadatas,
                ids=chunk_ids
            )

            logger.info(f"âœ… {len(chunks)}ê°œì˜ ìŠ¤ë§ˆíŠ¸ ì²­í¬ë¥¼ meeting_chunks DBì— ì €ì¥ ì™„ë£Œ (meeting_id: {meeting_id})")

        except Exception as e:
            logger.warning(f"âš ï¸ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.info(f"ğŸ“ ëŒ€ì‹  ê¸°ë³¸ ì²­í‚¹ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

            # ì—ëŸ¬ ë°œìƒ ì‹œ í´ë°±: RecursiveCharacterTextSplitter ì‚¬ìš©
            formatted_segments = []
            for seg in segments:
                speaker = seg.get('speaker_label', 'Unknown')
                start_time = seg.get('start_time', 0)
                text = seg.get('segment', '')
                minutes = int(start_time // 60)
                seconds = int(start_time % 60)
                time_str = f"{minutes:02d}:{seconds:02d}"
                formatted_text = f"[Speaker {speaker}, {time_str}] {text}"
                formatted_segments.append(formatted_text)

            full_text = "\n".join(formatted_segments)

            # RecursiveCharacterTextSplitterë¡œ ì²­í‚¹
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n[Speaker", "\n\n", "\n", " ", ""]
            )

            split_chunks = text_splitter.split_text(full_text)

            # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì •ì œ
            logger.info(f"ğŸ”§ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ í´ë°± í…ìŠ¤íŠ¸ ì •ì œ ì¤‘...")
            cleaned_chunks = [self._clean_text(chunk) for chunk in split_chunks]

            chunk_texts = []
            chunk_metadatas = []
            chunk_ids = []

            for i, chunk_text in enumerate(cleaned_chunks):
                chunk_texts.append(chunk_text)
                # meeting_dateë¥¼ ë¬¸ìì—´ë¡œ ê°•ì œ ë³€í™˜ (datetime ê°ì²´ì¼ ê²½ìš° ëŒ€ë¹„)
                meeting_date_str = str(meeting_date) if meeting_date else ""
                chunk_metadatas.append({
                    "meeting_id": meeting_id,
                    "dialogue_id": f"{meeting_id}_chunk_{i}",
                    "chunk_index": i,
                    "title": title,
                    "meeting_date": meeting_date_str,
                    "audio_file": audio_file
                })
                chunk_ids.append(f"{meeting_id}_chunk_{i}")

            chunk_vdb.add_texts(
                texts=chunk_texts,
                metadatas=chunk_metadatas,
                ids=chunk_ids
            )

            logger.info(f"âœ… {len(split_chunks)}ê°œì˜ ì²­í¬ë¥¼ meeting_chunks DBì— ì €ì¥ ì™„ë£Œ (í´ë°± ëª¨ë“œ)")

    def _create_smart_chunks(self, segments, max_chunk_size=1000, time_gap_threshold=60):
        """
        í™”ì ë³€ê²½, ì‹œê°„ ê°„ê²©ì„ ê³ ë ¤í•œ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹

        Args:
            segments (list): íšŒì˜ ëŒ€í™” ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
            max_chunk_size (int): ìµœëŒ€ ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
            time_gap_threshold (int): ì‹œê°„ ê°„ê²© ì„ê³„ê°’ (ì´ˆ)

        Returns:
            list: ì²­í¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{'text': str, 'start_time': float, 'end_time': float, 'speaker_count': int}]
        """
        chunks = []
        current_chunk = []
        current_chunk_text = ""
        current_speaker = None
        last_time = 0
        speakers_in_chunk = set()

        for seg in segments:
            speaker = seg.get('speaker_label', 'Unknown')
            start_time = seg.get('start_time', 0)
            text = seg.get('segment', '')

            # ì‹œê°„ì„ MM:SS í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            minutes = int(start_time // 60)
            seconds = int(start_time % 60)
            time_str = f"{minutes:02d}:{seconds:02d}"

            # í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸
            formatted_text = f"[Speaker {speaker}, {time_str}] {text}"

            # ì²­í¬ ë¶„ë¦¬ ì¡°ê±´:
            # 1. í˜„ì¬ ì²­í¬ í¬ê¸°ê°€ max_chunk_sizeë¥¼ ì´ˆê³¼
            # 2. ì‹œê°„ ê°„ê²©ì´ time_gap_threshold ì´ˆê³¼ (ê¸´ ì¹¨ë¬µì´ë‚˜ ì£¼ì œ ì „í™˜ ê°€ëŠ¥ì„±)
            # 3. í™”ìê°€ ë³€ê²½ë˜ê³  í˜„ì¬ ì²­í¬ê°€ ì¶©ë¶„íˆ í¼ (500ì ì´ìƒ)

            time_gap = start_time - last_time
            should_split = False

            if len(current_chunk_text) + len(formatted_text) > max_chunk_size:
                should_split = True
            elif time_gap > time_gap_threshold and len(current_chunk_text) > 200:
                should_split = True
            elif speaker != current_speaker and len(current_chunk_text) > 500:
                # í™”ì ë³€ê²½ ì‹œ ì ë‹¹í•œ í¬ê¸°ë©´ ë¶„ë¦¬
                should_split = True

            if should_split and current_chunk:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                chunks.append({
                    'text': current_chunk_text.strip(),
                    'start_time': current_chunk[0].get('start_time', 0),
                    'end_time': current_chunk[-1].get('start_time', 0),
                    'speaker_count': len(speakers_in_chunk)
                })

                # ìƒˆ ì²­í¬ ì‹œì‘
                current_chunk = []
                current_chunk_text = ""
                speakers_in_chunk = set()

            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€
            current_chunk.append(seg)
            current_chunk_text += formatted_text + "\n"
            speakers_in_chunk.add(speaker)
            current_speaker = speaker
            last_time = start_time

        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunks.append({
                'text': current_chunk_text.strip(),
                'start_time': current_chunk[0].get('start_time', 0),
                'end_time': current_chunk[-1].get('start_time', 0),
                'speaker_count': len(speakers_in_chunk)
            })

        return chunks


    def add_meeting_as_subtopic(self, meeting_id, title, meeting_date, audio_file, summary_content):
        """ìŠ¤í¬ë¦½íŠ¸ ì „ì²´ë¥¼ ì†Œì£¼ì œë³„ ì²­í¬ë¡œ DBì— ì €ì¥í•©ë‹ˆë‹¤."""

        
        # 1. ìƒì„±ëœ ìš”ì•½ì„ ì£¼ì œë³„ë¡œ íŒŒì‹±
        # "### "ë¡œ ë¶„ë¦¬í•˜ë˜, ì²« ë²ˆì§¸ ìš”ì†Œê°€ ê³µë°±ì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ filter(None, ...) ì‚¬ìš©
        summary_chunks = summary_content.split('\n### ')
        summary_chunks = [chunk.strip() for chunk in summary_chunks if chunk.strip()]
        
        # ì²« ë²ˆì§¸ ì²­í¬ì— "### "ê°€ ëˆ„ë½ë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì²« ë²ˆì§¸ ì²­í¬ë§Œ ë”°ë¡œ ì²˜ë¦¬
        # if summary_chunks and not summary_chunks[0].startswith('###'):
        #      # ì²«ë²ˆì§¸ ì²­í¬ê°€ ###ë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ###ë¥¼ ë¶™ì—¬ì¤€ë‹¤.
        #      if summary_chunks[0].count('\n') > 0:
        #          summary_chunks[0] = '### ' + summary_chunks[0]

        logger.info("===============summary_chunks=================")
        logger.info(summary_chunks)
        
        # 2. ê° ìš”ì•½ chunkë¥¼ Summary_Analysis_DBì— ì €ì¥
        subtopic_vdb = self.vectorstores['subtopic']
        chunk_texts = []
        chunk_metadatas = []
        chunk_ids = []

        for i, chunk in enumerate(summary_chunks):
            # '### 'ê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬, ì²« ì¤„ì„ main_topicìœ¼ë¡œ ì¶”ì¶œ
            lines = chunk.split('\n')
            main_topic = lines[0].replace('### ', '').strip()

            # ì‹¤ì œ ì €ì¥ë  ë‚´ìš©ì€ '### 'ë¥¼ í¬í•¨í•œ ì „ì²´ ì²­í¬
            full_chunk_content = '### ' + chunk if not chunk.startswith('###') else chunk

            # meeting_dateë¥¼ ë¬¸ìì—´ë¡œ ê°•ì œ ë³€í™˜ (datetime ê°ì²´ì¼ ê²½ìš° ëŒ€ë¹„)
            meeting_date_str = str(meeting_date) if meeting_date else ""

            chunk_texts.append(full_chunk_content)
            chunk_metadatas.append({
                "meeting_id": meeting_id,
                "meeting_title": title,
                "meeting_date": meeting_date_str,
                "audio_file": audio_file,
                "main_topic": main_topic,
                "summary_index": i
            })
            chunk_ids.append(f"{meeting_id}_summary_{i}")

        if chunk_texts:
            subtopic_vdb.add_texts(texts=chunk_texts, metadatas=chunk_metadatas, ids=chunk_ids)
            logger.info(f"ğŸ“„ ìš”ì•½ ê²°ê³¼ {len(chunk_texts)}ê°œë¥¼ Summary_Analysis_DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            return summary_chunks
        else:
            logger.warning("âš ï¸ ìš”ì•½ ê²°ê³¼ì—ì„œ ìœ íš¨í•œ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")



    
    
    def search(self,
             db_type: str,
             query: str,
             k: int = 5,
             retriever_type: str = "similarity",
             filter_criteria: dict = None,
             score_threshold: float = None,  # <-- [ìˆ˜ì •ë¨] ì ìˆ˜ ì„ê³„ê°’ ì¶”ê°€
             mmr_fetch_k: int = 20,         # <-- [ìˆ˜ì •ë¨] MMR fetch_k ì¶”ê°€
             mmr_lambda_mult: float = 0.5   # <-- [ìˆ˜ì •ë¨] MMR lambda_mult ì¶”ê°€
             ) -> list:
        """
        ì§€ì •ëœ DBì—ì„œ ì¿¼ë¦¬ì™€ í•„í„° ì¡°ê±´ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        score_thresholdê°€ ì§€ì •ë˜ë©´ retriever_typeì€ 'similarity_score_threshold'ë¡œ ìë™ ë³€ê²½ë©ë‹ˆë‹¤.

        Args:
            db_type (str): ê²€ìƒ‰í•  DB íƒ€ì… ('chunks', 'subtopic').
            query (str): ê²€ìƒ‰í•  í…ìŠ¤íŠ¸ ì¿¼ë¦¬.
            k (int, optional): ë°˜í™˜í•  ê²°ê³¼ì˜ ìˆ˜. Defaults to 5.
            retriever_type (str, optional): ì‚¬ìš©í•  ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì… ('similarity', 'mmr', 'self_query', 'similarity_score_threshold'). Defaults to "similarity".
            filter_criteria (dict, optional): ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì¡°ê±´ (ì˜ˆ: {'meeting_id': '...', 'audio_file': '...'}). Defaults to None.
            score_threshold (float, optional): ìœ ì‚¬ë„ ì ìˆ˜ ì„ê³„ê°’ (0.0~1.0). Defaults to None.
            mmr_fetch_k (int, optional): MMRì—ì„œ ì´ˆê¸° fetchí•  ë¬¸ì„œ ìˆ˜. Defaults to 20.
            mmr_lambda_mult (float, optional): MMRì˜ ë‹¤ì–‘ì„± íŒŒë¼ë¯¸í„° (0.0~1.0). Defaults to 0.5.

        Returns:
            list: LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸.
        """
        # 1. Validate inputs
        if db_type not in self.vectorstores:
            raise ValueError(f"Unknown db_type: {db_type}. Available types are {list(self.vectorstores.keys())}")

        # [ìˆ˜ì •ë¨] "similarity_score_threshold"ë¥¼ ìœ íš¨í•œ íƒ€ì…ìœ¼ë¡œ í—ˆìš©
        allowed_types = ["similarity", "mmr", "self_query", "similarity_score_threshold"]
        if retriever_type not in allowed_types:
            raise ValueError(f"Unsupported retriever_type: {retriever_type}. Choose from {allowed_types}.")

        # [ìˆ˜ì •ë¨] score_thresholdê°€ ì œê³µë˜ë©´, retriever_typeì„ ê°•ì œë¡œ ë³€ê²½
        current_retriever_type = retriever_type
        if score_threshold is not None and retriever_type == "similarity":
            current_retriever_type = "similarity_score_threshold"
            logger.info(f"â„¹ï¸ score_threshold provided. Changing retriever_type to 'similarity_score_threshold'.")

        vdb = self.vectorstores[db_type]
        results = []

        # 2. Handle 'similarity', 'mmr', 'similarity_score_threshold' retrievers
        if current_retriever_type in ["similarity", "mmr", "similarity_score_threshold"]:
            search_kwargs = {'k': k}
            if filter_criteria:
                search_kwargs['filter'] = filter_criteria

            # [ìˆ˜ì •ë¨] íƒ€ì…ì— ë”°ë¼ search_kwargs ë™ì  êµ¬ì„±
            if current_retriever_type == "similarity_score_threshold":
                if score_threshold is None:
                    raise ValueError("score_threshold must be provided when retriever_type is 'similarity_score_threshold'")
                search_kwargs['score_threshold'] = score_threshold

            elif current_retriever_type == "mmr":
                search_kwargs['fetch_k'] = mmr_fetch_k
                search_kwargs['lambda_mult'] = mmr_lambda_mult

            retriever = vdb.as_retriever(
                search_type=current_retriever_type,
                search_kwargs=search_kwargs
            )
            results = retriever.invoke(query)

        # 3. Handle 'self_query' retriever
        elif current_retriever_type == "self_query":
            # (ì°¸ê³ : SelfQueryRetrieverëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë‚´ë¶€ì—ì„œ similarity_searchë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.)
            # (ì—¬ê¸°ì„œ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§ì„ í•˜ë ¤ë©´, SelfQueryRetrieverë¥¼ ì»¤ìŠ¤í…€í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.)
            metadata_info = self.metadata_field_infos[db_type]
            doc_description = self.document_content_descriptions[db_type]

            try:
                retriever = SelfQueryRetriever.from_llm(
                    self.llm,
                    vdb,
                    doc_description,
                    metadata_info,
                    verbose=True,
                    base_filter=filter_criteria,
                    # [ê°œì„  ì•„ì´ë””ì–´] SelfQueryRetrieverê°€ ë°˜í™˜í•  kì˜ ê°œìˆ˜ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                    # ë‹¤ë§Œ, invoke ì‹œì ì´ ì•„ë‹Œ ìƒì„± ì‹œì ì— search_kwargsë¥¼ ë„˜ê²¨ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (LangChain ë²„ì „ì— ë”°ë¼ ë‹¤ë¦„)
                    # enable_limit=Trueë¥¼ ì‚¬ìš©í•˜ê³  ì¿¼ë¦¬ì— "top 3 results" ë“±ì„ í¬í•¨ì‹œì¼œì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
                )
                results = retriever.invoke(query)

                # [ìˆ˜ì •ë¨] SelfQuery ì´í›„ì—ë„ kê°œë§Œ ë°˜í™˜í•˜ë„ë¡ ê°•ì œ (í•„ìš”ì‹œ)
                # SelfQueryRetrieverëŠ” kë¥¼ LLMì´ ì¶”ë¡ í•˜ê²Œ í•˜ë¯€ë¡œ, kê°€ ë¬´ì‹œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                # ë§Œì•½ ê²°ê³¼ê°€ ë„ˆë¬´ ë§ë‹¤ë©´, kë§Œí¼ ì˜ë¼ëƒ…ë‹ˆë‹¤.
                if len(results) > k:
                     logger.info(f"â„¹ï¸ SelfQuery found {len(results)} results. Truncating to k={k}.")
                     results = results[:k]

            except Exception as e:
                # SelfQuery ì‹¤íŒ¨ ì‹œ similarity searchë¡œ í´ë°±
                error_msg = str(e)
                logger.warning(f"âš ï¸ SelfQuery ì‹¤íŒ¨ (í´ë°±: similarity search): {error_msg}")

                # ChromaDB í˜¸í™˜ë˜ì§€ ì•ŠëŠ” í•„í„° ì˜¤ë¥˜ì¸ ê²½ìš°, similarity searchë¡œ ëŒ€ì²´
                if "Expected where operand value" in error_msg or "type" in error_msg:
                    logger.warning("   â†’ ChromaDB í˜¸í™˜ë˜ì§€ ì•ŠëŠ” í•„í„° í˜•ì‹ ê°ì§€. similarity searchë¡œ ì „í™˜í•©ë‹ˆë‹¤.")

                search_kwargs = {'k': k}
                if filter_criteria:
                    search_kwargs['filter'] = filter_criteria

                retriever = vdb.as_retriever(
                    search_type="similarity",
                    search_kwargs=search_kwargs
                )
                results = retriever.invoke(query)

        logger.info(f"âœ… Found {len(results)} documents from '{self.COLLECTION_NAMES[db_type]}' for query: '{query}'")
        return results

    
    def get_chunks_by_meeting_id(self, meeting_id: str) -> str:
        """
        meeting_idë¡œ ì²­í‚¹ëœ ë¬¸ì„œë¥¼ chunk_index ìˆœì„œëŒ€ë¡œ ê°€ì ¸ì™€ì„œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©í•©ë‹ˆë‹¤.

        Args:
            meeting_id (str): íšŒì˜ ID

        Returns:
            str: chunk_index ìˆœì„œëŒ€ë¡œ ê²°í•©ëœ ì „ì²´ ì²­í¬ í…ìŠ¤íŠ¸
                 (ì²­í¬ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜)
        """
        try:
            # meeting_chunks ì»¬ë ‰ì…˜ì—ì„œ í•´ë‹¹ meeting_idì˜ ëª¨ë“  ì²­í¬ ì¡°íšŒ
            collection = self.client.get_collection(name=self.COLLECTION_NAMES['chunks'])

            # meeting_idë¡œ í•„í„°ë§í•˜ì—¬ ëª¨ë“  í•­ëª© ê°€ì ¸ì˜¤ê¸°
            results = collection.get(
                where={"meeting_id": meeting_id},
                include=["documents", "metadatas"]
            )

            if not results or not results.get('documents'):
                logger.warning(f"âš ï¸ meeting_id '{meeting_id}'ì— ëŒ€í•œ ì²­í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return ""

            # documentsì™€ metadatasë¥¼ chunk_index ìˆœì„œë¡œ ì •ë ¬
            documents = results['documents']
            metadatas = results['metadatas']

            # (chunk_index, document) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„± í›„ ì •ë ¬
            indexed_docs = []
            for doc, meta in zip(documents, metadatas):
                chunk_index = meta.get('chunk_index', 0)
                indexed_docs.append((chunk_index, doc))

            # chunk_index ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            indexed_docs.sort(key=lambda x: x[0])

            # ë¬¸ì„œë“¤ì„ ìˆœì„œëŒ€ë¡œ ê²°í•© (ê° ë¬¸ì„œ ì‚¬ì´ì— ì¤„ë°”ê¿ˆ 2ê°œ ì¶”ê°€)
            full_chunks = "\n\n".join([doc for _, doc in indexed_docs])

            logger.info(f"âœ… meeting_id '{meeting_id}'ì— ëŒ€í•œ {len(indexed_docs)}ê°œì˜ ì²­í¬ë¥¼ ìˆœì„œëŒ€ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            return full_chunks

        except Exception as e:
            logger.error(f"âŒ ì²­í¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return ""

    def get_summary_by_meeting_id(self, meeting_id: str) -> str:
        """
        meeting_idë¡œ ë¬¸ë‹¨ ìš”ì•½ì„ summary_index ìˆœì„œëŒ€ë¡œ ê°€ì ¸ì™€ì„œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©í•©ë‹ˆë‹¤.

        Args:
            meeting_id (str): íšŒì˜ ID

        Returns:
            str: summary_index ìˆœì„œëŒ€ë¡œ ê²°í•©ëœ ì „ì²´ ë¬¸ë‹¨ ìš”ì•½ í…ìŠ¤íŠ¸
                 (ìš”ì•½ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜)
        """
        try:
            # meeting_subtopic ì»¬ë ‰ì…˜ì—ì„œ í•´ë‹¹ meeting_idì˜ ëª¨ë“  ì²­í¬ ì¡°íšŒ
            collection = self.client.get_collection(name=self.COLLECTION_NAMES['subtopic'])

            # meeting_idë¡œ í•„í„°ë§í•˜ì—¬ ëª¨ë“  í•­ëª© ê°€ì ¸ì˜¤ê¸°
            results = collection.get(
                where={"meeting_id": meeting_id},
                include=["documents", "metadatas"]
            )

            if not results or not results.get('documents'):
                logger.warning(f"âš ï¸ meeting_id '{meeting_id}'ì— ëŒ€í•œ ë¬¸ë‹¨ ìš”ì•½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return ""

            # documentsì™€ metadatasë¥¼ summary_index ìˆœì„œë¡œ ì •ë ¬
            documents = results['documents']
            metadatas = results['metadatas']

            # (summary_index, document) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„± í›„ ì •ë ¬
            indexed_docs = []
            for doc, meta in zip(documents, metadatas):
                summary_index = meta.get('summary_index', 0)
                indexed_docs.append((summary_index, doc))

            # summary_index ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            indexed_docs.sort(key=lambda x: x[0])

            # ë¬¸ì„œë“¤ì„ ìˆœì„œëŒ€ë¡œ ê²°í•© (ê° ë¬¸ì„œ ì‚¬ì´ì— ì¤„ë°”ê¿ˆ 2ê°œ ì¶”ê°€)
            full_summary = "\n\n".join([doc for _, doc in indexed_docs])

            logger.info(f"âœ… meeting_id '{meeting_id}'ì— ëŒ€í•œ {len(indexed_docs)}ê°œì˜ ë¬¸ë‹¨ ìš”ì•½ì„ ìˆœì„œëŒ€ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            return full_summary

        except Exception as e:
            logger.error(f"âŒ ë¬¸ë‹¨ ìš”ì•½ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return ""

    def delete_from_collection(self, db_type, meeting_id=None, audio_file=None, title=None):
        """
        ì§€ì •ëœ ë²¡í„° DB ì»¬ë ‰ì…˜ì—ì„œ í•­ëª©ì„ ì‚­ì œí•©ë‹ˆë‹¤.
        db_typeì´ "all"ì´ë©´ SQLite, Vector DB (chunks + subtopic), ì˜¤ë””ì˜¤ íŒŒì¼ì„ ëª¨ë‘ ì‚­ì œí•©ë‹ˆë‹¤.
        meeting_id, audio_file, title ì¤‘ í•˜ë‚˜ ì´ìƒì´ ì œê³µë˜ë©´ í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” í•­ëª©ì„ ì‚­ì œí•©ë‹ˆë‹¤.
        ì•„ë¬´ê²ƒë„ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ í•´ë‹¹ db_typeì˜ ì „ì²´ ì»¬ë ‰ì…˜ì„ ì‚­ì œí•©ë‹ˆë‹¤.
        """
        # db_typeì´ "all"ì´ë©´ ëª¨ë“  ë°ì´í„° ì‚­ì œ (SQLite + Vector DB + ì˜¤ë””ì˜¤ íŒŒì¼)
        if db_type == "all":
            if not meeting_id:
                raise ValueError("meeting_id is required when db_type is 'all'")
            return self._delete_all_meeting_data(meeting_id)

        # ê¸°ì¡´ ë¡œì§: íŠ¹ì • ì»¬ë ‰ì…˜ë§Œ ì‚­ì œ
        if db_type not in self.vectorstores:
            raise ValueError(f"Unknown db_type: {db_type}. Must be one of {list(self.COLLECTION_NAMES.keys())}")

        collection = self.client.get_or_create_collection(name=self.COLLECTION_NAMES[db_type])

        filters = {}
        if meeting_id:
            filters["meeting_id"] = meeting_id
        if audio_file:
            filters["audio_file"] = audio_file
        if title:
            filters["title"] = title

        if filters:
            # íŠ¹ì • í•„í„°ê°€ ìˆëŠ” ê²½ìš°
            logger.info(f"ğŸ—‘ï¸ Deleting from '{db_type}' collection with filters: {filters}")
            collection.delete(where=filters)
            logger.info(f"âœ… Deletion from '{db_type}' collection complete.")
        else:
            # í•„í„°ê°€ ì—†ëŠ” ê²½ìš°, ì „ì²´ ì»¬ë ‰ì…˜ ì‚­ì œ
            logger.warning(f"âš ï¸ No specific filters provided. Deleting ALL items from '{db_type}' collection.")
            collection.delete(where={}) # deletes all items
            logger.info(f"âœ… All items deleted from '{db_type}' collection.")

    def _get_audio_file_from_vector_db(self, meeting_id):
        """
        Vector DBì—ì„œ meeting_idë¡œ audio_fileì„ ì¡°íšŒí•©ë‹ˆë‹¤.
        SQLiteì—ì„œ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ í´ë°±ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

        Args:
            meeting_id (str): íšŒì˜ ID

        Returns:
            str or None: audio_file ì´ë¦„ ë˜ëŠ” None
        """
        try:
            import sqlite3
            conn = sqlite3.connect(os.path.join(os.path.dirname(__file__), '..', 'database', 'vector_db', 'chroma.sqlite3'))
            cursor = conn.cursor()

            cursor.execute('''
                SELECT string_value
                FROM embedding_metadata
                WHERE key = "audio_file" AND id IN (
                    SELECT DISTINCT id FROM embedding_metadata
                    WHERE key = "meeting_id" AND string_value = ?
                )
                LIMIT 1
            ''', (meeting_id,))

            result = cursor.fetchone()
            conn.close()

            return result[0] if result else None
        except Exception as e:
            logger.warning(f"âš ï¸ Vector DBì—ì„œ audio_file ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def _delete_all_meeting_data(self, meeting_id):
        """
        meeting_idë¡œ ëª¨ë“  íšŒì˜ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
        - SQLite DB: meeting_dialogues, meeting_minutes
        - Vector DB: meeting_chunks, meeting_subtopic
        - ì˜¤ë””ì˜¤ íŒŒì¼

        Args:
            meeting_id (str): ì‚­ì œí•  íšŒì˜ ID

        Returns:
            dict: ì‚­ì œ ê²°ê³¼ ì •ë³´
        """
        # db_managerê°€ ì£¼ì…ë˜ì§€ ì•Šì€ ê²½ìš° ì—ëŸ¬ ë°œìƒ
        if not self.db_manager:
            raise ValueError("DatabaseManager instance is required for deleting all meeting data. Please set db_manager in VectorDBManager constructor.")

        # ì‚­ì œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ë¡œê·¸
        logger.info("\n\n" + "=" * 70)
        logger.info(f"ğŸ—‘ï¸  [íšŒì˜ ë°ì´í„° ì‚­ì œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘]")
        logger.info("=" * 70)
        logger.info(f"ğŸ”‘ ì‚­ì œ í‚¤ê°’(meeting_id): {meeting_id}")
        logger.info(f"ğŸ“ ì´ í‚¤ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì‚­ì œí•©ë‹ˆë‹¤:")
        logger.info(f"   â€¢ SQLite DB - meeting_dialogues í…Œì´ë¸” (WHERE meeting_id = '{meeting_id}')")
        logger.info(f"   â€¢ SQLite DB - meeting_minutes í…Œì´ë¸” (WHERE meeting_id = '{meeting_id}')")
        logger.info(f"   â€¢ Vector DB - meeting_chunk ì»¬ë ‰ì…˜ (WHERE meeting_id = '{meeting_id}')")
        logger.info(f"   â€¢ Vector DB - meeting_subtopic ì»¬ë ‰ì…˜ (WHERE meeting_id = '{meeting_id}')")
        logger.info(f"   â€¢ ë¯¸ë””ì–´ íŒŒì¼ (ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤, uploads í´ë”)")
        logger.info("=" * 70)

        # 1. meeting_idë¡œ ì˜¤ë””ì˜¤ íŒŒì¼ëª… ì¡°íšŒ (SQLite ìš°ì„ , ì‹¤íŒ¨ ì‹œ Vector DB)
        audio_file = self.db_manager.get_audio_file_by_meeting_id(meeting_id)

        if not audio_file:
            logger.warning("âš ï¸ SQLiteì—ì„œ audio_fileì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Vector DBì—ì„œ ì¡°íšŒ ì‹œë„...")
            audio_file = self._get_audio_file_from_vector_db(meeting_id)

        if not audio_file:
            logger.warning("âš ï¸ audio_fileì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ì‚­ì œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            logger.warning("   (SQLiteì™€ Vector DB ì‚­ì œëŠ” ê³„ì† ì§„í–‰ë©ë‹ˆë‹¤)")
            audio_file = None  # íŒŒì¼ ì‚­ì œ ë‹¨ê³„ì—ì„œ ì²˜ë¦¬
        else:
            logger.info(f"ğŸ“„ ë¯¸ë””ì–´ íŒŒì¼ëª…: {audio_file}")

        logger.info("=" * 70)

        # 2. SQLite DB ì‚­ì œ
        deleted_sqlite = self.db_manager.delete_meeting_by_id(meeting_id)

        # 3. Vector DB chunks ì‚­ì œ
        deleted_chunks_count = 0
        before_chunks_count = 0
        after_chunks_count = 0
        try:
            logger.info(f"\nğŸ“Š [Vector DB Chunks ì‚­ì œ ê²€ì¦ ì‹œì‘] meeting_id = {meeting_id}")
            logger.info("=" * 70)

            # LangChain vectorstoreì˜ underlying collection ì‚¬ìš©
            chunks_collection = self.vectorstores['chunks']._collection

            # ì‚­ì œ ì „ ì „ì²´ ë°ì´í„° ê°œìˆ˜ í™•ì¸ (limit ì—†ì´ ì¡°íšŒ)
            before_delete = chunks_collection.get(where={"meeting_id": meeting_id})
            if before_delete and before_delete.get('ids'):
                before_chunks_count = len(before_delete['ids'])
                logger.info(f"[ì‚­ì œ ì „] meeting_chunk: {before_chunks_count}ê°œ")
                logger.info("-" * 70)

                # ì‚­ì œ ì‹¤í–‰
                chunks_collection.delete(where={"meeting_id": meeting_id})
                logger.info(f"[ì‚­ì œ ìˆ˜í–‰] meeting_chunk: {before_chunks_count}ê°œ ì‚­ì œ ì‹œë„")
                deleted_chunks_count = before_chunks_count

                logger.info("-" * 70)

                # ì‚­ì œ í›„ í™•ì¸
                after_delete = chunks_collection.get(where={"meeting_id": meeting_id})
                if after_delete and after_delete.get('ids'):
                    after_chunks_count = len(after_delete['ids'])
                    logger.info(f"[ì‚­ì œ í›„] meeting_chunk: {after_chunks_count}ê°œ ë‚¨ìŒ")
                    logger.warning(f"âš ï¸ Vector DB (meeting_chunk) ì‚­ì œ ê²€ì¦ ì‹¤íŒ¨: {after_chunks_count}ê°œ ë°ì´í„°ê°€ ë‚¨ì•„ìˆìŒ")
                else:
                    logger.info(f"[ì‚­ì œ í›„] meeting_chunk: 0ê°œ ë‚¨ìŒ")
                    logger.info(f"âœ… Vector DB (meeting_chunk) ì‚­ì œ ê²€ì¦ ì„±ê³µ: ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logger.info(f"[ì‚­ì œ ì „] meeting_chunk: 0ê°œ")
                logger.info(f"â„¹ï¸ Vector DB (meeting_chunk)ì— meeting_id={meeting_id} ë°ì´í„° ì—†ìŒ")

            logger.info("=" * 70)

        except Exception as e:
            logger.error(f"âŒ Vector DB (meeting_chunk) ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()

        # 4. Vector DB subtopic ì‚­ì œ
        deleted_subtopic_count = 0
        before_subtopic_count = 0
        after_subtopic_count = 0
        try:
            logger.info(f"\nğŸ“Š [Vector DB Subtopic ì‚­ì œ ê²€ì¦ ì‹œì‘] meeting_id = {meeting_id}")
            logger.info("=" * 70)

            # LangChain vectorstoreì˜ underlying collection ì‚¬ìš©
            subtopic_collection = self.vectorstores['subtopic']._collection

            # ì‚­ì œ ì „ ì „ì²´ ë°ì´í„° ê°œìˆ˜ í™•ì¸ (limit ì—†ì´ ì¡°íšŒ)
            before_delete = subtopic_collection.get(where={"meeting_id": meeting_id})
            if before_delete and before_delete.get('ids'):
                before_subtopic_count = len(before_delete['ids'])
                logger.info(f"[ì‚­ì œ ì „] meeting_subtopic: {before_subtopic_count}ê°œ")
                logger.info("-" * 70)

                # ì‚­ì œ ì‹¤í–‰
                subtopic_collection.delete(where={"meeting_id": meeting_id})
                logger.info(f"[ì‚­ì œ ìˆ˜í–‰] meeting_subtopic: {before_subtopic_count}ê°œ ì‚­ì œ ì‹œë„")
                deleted_subtopic_count = before_subtopic_count

                logger.info("-" * 70)

                # ì‚­ì œ í›„ í™•ì¸
                after_delete = subtopic_collection.get(where={"meeting_id": meeting_id})
                if after_delete and after_delete.get('ids'):
                    after_subtopic_count = len(after_delete['ids'])
                    logger.info(f"[ì‚­ì œ í›„] meeting_subtopic: {after_subtopic_count}ê°œ ë‚¨ìŒ")
                    logger.warning(f"âš ï¸ Vector DB (meeting_subtopic) ì‚­ì œ ê²€ì¦ ì‹¤íŒ¨: {after_subtopic_count}ê°œ ë°ì´í„°ê°€ ë‚¨ì•„ìˆìŒ")
                else:
                    logger.info(f"[ì‚­ì œ í›„] meeting_subtopic: 0ê°œ ë‚¨ìŒ")
                    logger.info(f"âœ… Vector DB (meeting_subtopic) ì‚­ì œ ê²€ì¦ ì„±ê³µ: ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logger.info(f"[ì‚­ì œ ì „] meeting_subtopic: 0ê°œ")
                logger.info(f"â„¹ï¸ Vector DB (meeting_subtopic)ì— meeting_id={meeting_id} ë°ì´í„° ì—†ìŒ")

            logger.info("=" * 70)

        except Exception as e:
            logger.error(f"âŒ Vector DB (meeting_subtopic) ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()

        # 5. ë¯¸ë””ì–´ íŒŒì¼ ì‚­ì œ (ì˜¤ë””ì˜¤ ë˜ëŠ” ë¹„ë””ì˜¤)
        logger.info(f"\nğŸ“Š [ë¯¸ë””ì–´ íŒŒì¼ ì‚­ì œ ê²€ì¦ ì‹œì‘] meeting_id = {meeting_id}")
        logger.info("=" * 70)

        audio_deleted = False

        if audio_file:
            audio_path = os.path.join(self.upload_folder, audio_file)

            if os.path.exists(audio_path):
                logger.info(f"[ì‚­ì œ ì „] ë¯¸ë””ì–´ íŒŒì¼ ì¡´ì¬: {audio_file}")
                logger.info(f"           ê²½ë¡œ: {audio_path}")
                logger.info("-" * 70)

                os.remove(audio_path)
                logger.info(f"[ì‚­ì œ ìˆ˜í–‰] ë¯¸ë””ì–´ íŒŒì¼ ì‚­ì œ ì‹œë„: {audio_file}")

                logger.info("-" * 70)

                if not os.path.exists(audio_path):
                    logger.info(f"[ì‚­ì œ í›„] ë¯¸ë””ì–´ íŒŒì¼ ì—†ìŒ")
                    logger.info(f"âœ… ë¯¸ë””ì–´ íŒŒì¼ ì‚­ì œ ê²€ì¦ ì„±ê³µ: íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    audio_deleted = True
                else:
                    logger.info(f"[ì‚­ì œ í›„] ë¯¸ë””ì–´ íŒŒì¼ ì—¬ì „íˆ ì¡´ì¬")
                    logger.warning(f"âš ï¸ ë¯¸ë””ì–´ íŒŒì¼ ì‚­ì œ ê²€ì¦ ì‹¤íŒ¨: íŒŒì¼ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
            else:
                logger.info(f"[ì‚­ì œ ì „] ë¯¸ë””ì–´ íŒŒì¼ ì—†ìŒ: {audio_file}")
                logger.info(f"â„¹ï¸ ë¯¸ë””ì–´ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            logger.info(f"[ê±´ë„ˆëœ€] audio_file ì •ë³´ ì—†ìŒ")
            logger.info(f"â„¹ï¸ audio_fileì„ ì°¾ì„ ìˆ˜ ì—†ì–´ íŒŒì¼ ì‚­ì œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

        logger.info("=" * 70)

        # ìµœì¢… ìš”ì•½
        logger.info(f"\n{'=' * 70}")
        logger.info(f"ğŸ‰ [ì‚­ì œ ì‘ì—… ìµœì¢… ìš”ì•½] meeting_id = {meeting_id}")
        logger.info("=" * 70)
        logger.info(f"âœ“ SQLite meeting_dialogues: {deleted_sqlite['dialogues']}ê°œ ì‚­ì œ")
        logger.info(f"âœ“ SQLite meeting_minutes: {deleted_sqlite['minutes']}ê°œ ì‚­ì œ")
        logger.info(f"âœ“ SQLite meeting_shares: {deleted_sqlite.get('shares', 0)}ê°œ ì‚­ì œ")
        logger.info(f"âœ“ Vector DB meeting_chunk: {deleted_chunks_count}ê°œ ì‚­ì œ")
        logger.info(f"âœ“ Vector DB meeting_subtopic: {deleted_subtopic_count}ê°œ ì‚­ì œ")
        logger.info(f"âœ“ ë¯¸ë””ì–´ íŒŒì¼ (ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤): {'ì‚­ì œë¨' if audio_deleted else 'ì—†ìŒ/ì‹¤íŒ¨'}")
        logger.info("=" * 70 + "\n")

        return {
            "success": True,
            "message": "íšŒì˜ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "deleted": {
                "sqlite_dialogues": deleted_sqlite["dialogues"],
                "sqlite_minutes": deleted_sqlite["minutes"],
                "sqlite_shares": deleted_sqlite.get("shares", 0),
                "vector_chunks": deleted_chunks_count,
                "vector_subtopic": deleted_subtopic_count,
                "audio_file": audio_file if audio_deleted else None
            }
        }

    def update_metadata_title(self, meeting_id, new_title):
        """
        ChromaDBì˜ meeting_chunkì™€ meeting_subtopic ì»¬ë ‰ì…˜ì—ì„œ
        í•´ë‹¹ meeting_idì˜ ëª¨ë“  ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì˜ titleì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

        Args:
            meeting_id (str): íšŒì˜ ID
            new_title (str): ìƒˆë¡œìš´ ì œëª©

        Returns:
            dict: ì—…ë°ì´íŠ¸ ê²°ê³¼ {'success': bool, 'updated_chunks': int, 'updated_subtopics': int}
        """
        logger.info(f"\nğŸ“Š [ChromaDB ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œì‘] meeting_id = {meeting_id}")
        logger.info("=" * 70)

        updated_chunks = 0
        updated_subtopics = 0

        try:
            # 1. meeting_chunk ì»¬ë ‰ì…˜ ì—…ë°ì´íŠ¸
            logger.info(f"[1/2] meeting_chunk ì»¬ë ‰ì…˜ ì—…ë°ì´íŠ¸ ì¤‘...")

            # ChromaDB ë„¤ì´í‹°ë¸Œ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
            chunk_collection = self.client.get_collection(name=self.COLLECTION_NAMES['chunks'])

            # meeting_idë¡œ ë¬¸ì„œ ì¡°íšŒ
            chunk_results = chunk_collection.get(
                where={"meeting_id": meeting_id}
            )

            chunk_ids = chunk_results['ids']

            if chunk_ids:
                # ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ titleë§Œ ë³€ê²½
                updated_metadatas = []
                for metadata in chunk_results['metadatas']:
                    updated_metadata = metadata.copy()
                    updated_metadata['title'] = new_title
                    updated_metadatas.append(updated_metadata)

                # ì¼ê´„ ì—…ë°ì´íŠ¸
                chunk_collection.update(
                    ids=chunk_ids,
                    metadatas=updated_metadatas
                )
                updated_chunks = len(chunk_ids)
                logger.info(f"   âœ… meeting_chunk: {updated_chunks}ê°œ ë¬¸ì„œì˜ title ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            else:
                logger.info(f"   â„¹ï¸ meeting_chunk: ì—…ë°ì´íŠ¸í•  ë¬¸ì„œ ì—†ìŒ")

            # 2. meeting_subtopic ì»¬ë ‰ì…˜ ì—…ë°ì´íŠ¸
            logger.info(f"[2/2] meeting_subtopic ì»¬ë ‰ì…˜ ì—…ë°ì´íŠ¸ ì¤‘...")

            # ChromaDB ë„¤ì´í‹°ë¸Œ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
            subtopic_collection = self.client.get_collection(name=self.COLLECTION_NAMES['subtopic'])

            # meeting_idë¡œ ë¬¸ì„œ ì¡°íšŒ
            subtopic_results = subtopic_collection.get(
                where={"meeting_id": meeting_id}
            )

            subtopic_ids = subtopic_results['ids']

            if subtopic_ids:
                # ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ meeting_titleë§Œ ë³€ê²½
                updated_metadatas = []
                for metadata in subtopic_results['metadatas']:
                    updated_metadata = metadata.copy()
                    updated_metadata['meeting_title'] = new_title  # â† meeting_subtopicì€ 'meeting_title' í•„ë“œ ì‚¬ìš©
                    updated_metadatas.append(updated_metadata)

                # ì¼ê´„ ì—…ë°ì´íŠ¸
                subtopic_collection.update(
                    ids=subtopic_ids,
                    metadatas=updated_metadatas
                )
                updated_subtopics = len(subtopic_ids)
                logger.info(f"   âœ… meeting_subtopic: {updated_subtopics}ê°œ ë¬¸ì„œì˜ meeting_title ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            else:
                logger.info(f"   â„¹ï¸ meeting_subtopic: ì—…ë°ì´íŠ¸í•  ë¬¸ì„œ ì—†ìŒ")

            logger.info("-" * 70)
            logger.info(f"âœ… ChromaDB ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            logger.info(f"   â€¢ meeting_chunk: {updated_chunks}ê°œ")
            logger.info(f"   â€¢ meeting_subtopic: {updated_subtopics}ê°œ")
            logger.info("=" * 70 + "\n")

            return {
                'success': True,
                'updated_chunks': updated_chunks,
                'updated_subtopics': updated_subtopics
            }

        except Exception as e:
            logger.error(f"âŒ ChromaDB ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            logger.info("=" * 70 + "\n")
            return {
                'success': False,
                'error': str(e),
                'updated_chunks': updated_chunks,
                'updated_subtopics': updated_subtopics
            }

    def update_metadata_date(self, meeting_id, new_date):
        """
        ChromaDBì˜ meeting_chunkì™€ meeting_subtopic ì»¬ë ‰ì…˜ì—ì„œ
        í•´ë‹¹ meeting_idì˜ ëª¨ë“  ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì˜ meeting_dateë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

        Args:
            meeting_id (str): íšŒì˜ ID
            new_date (str): ìƒˆë¡œìš´ ë‚ ì§œ (í˜•ì‹: "YYYY-MM-DD HH:MM:SS")

        Returns:
            dict: ì—…ë°ì´íŠ¸ ê²°ê³¼ {'success': bool, 'updated_chunks': int, 'updated_subtopics': int}
        """
        logger.info(f"\nğŸ“Š [ChromaDB ë‚ ì§œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œì‘] meeting_id = {meeting_id}")
        logger.info("=" * 70)

        updated_chunks = 0
        updated_subtopics = 0

        try:
            # 1. meeting_chunk ì»¬ë ‰ì…˜ ì—…ë°ì´íŠ¸
            logger.info(f"[1/2] meeting_chunk ì»¬ë ‰ì…˜ ì—…ë°ì´íŠ¸ ì¤‘...")

            # ChromaDB ë„¤ì´í‹°ë¸Œ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
            chunk_collection = self.client.get_collection(name=self.COLLECTION_NAMES['chunks'])

            # meeting_idë¡œ ë¬¸ì„œ ì¡°íšŒ
            chunk_results = chunk_collection.get(
                where={"meeting_id": meeting_id}
            )

            chunk_ids = chunk_results['ids']

            if chunk_ids:
                # ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ meeting_dateë§Œ ë³€ê²½
                updated_metadatas = []
                for metadata in chunk_results['metadatas']:
                    updated_metadata = metadata.copy()
                    updated_metadata['meeting_date'] = new_date
                    updated_metadatas.append(updated_metadata)

                # ì¼ê´„ ì—…ë°ì´íŠ¸
                chunk_collection.update(
                    ids=chunk_ids,
                    metadatas=updated_metadatas
                )
                updated_chunks = len(chunk_ids)
                logger.info(f"   âœ… meeting_chunk: {updated_chunks}ê°œ ë¬¸ì„œì˜ meeting_date ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            else:
                logger.info(f"   â„¹ï¸ meeting_chunk: ì—…ë°ì´íŠ¸í•  ë¬¸ì„œ ì—†ìŒ")

            # 2. meeting_subtopic ì»¬ë ‰ì…˜ ì—…ë°ì´íŠ¸
            logger.info(f"[2/2] meeting_subtopic ì»¬ë ‰ì…˜ ì—…ë°ì´íŠ¸ ì¤‘...")

            # ChromaDB ë„¤ì´í‹°ë¸Œ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
            subtopic_collection = self.client.get_collection(name=self.COLLECTION_NAMES['subtopic'])

            # meeting_idë¡œ ë¬¸ì„œ ì¡°íšŒ
            subtopic_results = subtopic_collection.get(
                where={"meeting_id": meeting_id}
            )

            subtopic_ids = subtopic_results['ids']

            if subtopic_ids:
                # ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ meeting_dateë§Œ ë³€ê²½
                updated_metadatas = []
                for metadata in subtopic_results['metadatas']:
                    updated_metadata = metadata.copy()
                    updated_metadata['meeting_date'] = new_date
                    updated_metadatas.append(updated_metadata)

                # ì¼ê´„ ì—…ë°ì´íŠ¸
                subtopic_collection.update(
                    ids=subtopic_ids,
                    metadatas=updated_metadatas
                )
                updated_subtopics = len(subtopic_ids)
                logger.info(f"   âœ… meeting_subtopic: {updated_subtopics}ê°œ ë¬¸ì„œì˜ meeting_date ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            else:
                logger.info(f"   â„¹ï¸ meeting_subtopic: ì—…ë°ì´íŠ¸í•  ë¬¸ì„œ ì—†ìŒ")

            logger.info("-" * 70)
            logger.info(f"âœ… ChromaDB ë‚ ì§œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            logger.info(f"   â€¢ meeting_chunk: {updated_chunks}ê°œ")
            logger.info(f"   â€¢ meeting_subtopic: {updated_subtopics}ê°œ")
            logger.info("=" * 70 + "\n")

            return {
                'success': True,
                'updated_chunks': updated_chunks,
                'updated_subtopics': updated_subtopics
            }

        except Exception as e:
            logger.error(f"âŒ ChromaDB ë‚ ì§œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            logger.info("=" * 70 + "\n")
            return {
                'success': False,
                'error': str(e),
                'updated_chunks': updated_chunks,
                'updated_subtopics': updated_subtopics
            }



# --- ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ---
# DB íŒŒì¼ì€ minute_ai/database/vector_db ê²½ë¡œì— ì €ì¥ë©ë‹ˆë‹¤.
# vector_db_path = os.path.join(os.path.dirname(__file__), '..', 'database', 'vector_db')
upload_folder_path = os.path.join(os.path.dirname(__file__), '..', 'uploads')
vdb_manager = VectorDBManager(upload_folder=upload_folder_path)
